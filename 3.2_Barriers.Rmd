# Barrier Detection {.imageChapter}

<div class="chapter_image"><img src="media/ch_palm.jpg"></div>

> Barriers in the landscape cause a reduction in the flow of individuals or gametes and can be using genetic data in a couple of different ways.  In this section, we begin with exploring the notion of admixture and how that can be leveraged to understand physical barriers on the landscape using a population genetic framework in a program called STRUCTURE. We will also explore some unsupervised clustering approaches that may also provide valuable insights into how spatial genetic data may be partitioned on the landscape.


## Admixture

Admixture is the mixing of two or more populations whose frequency spectra may differ.  If they differ considerably, then the admixed population may be easily examined to determine relative contributions of each population.  In a previous chapter, we covered a phenomenon called the Wahlund Effect (Chapter \@ref(wahlund-effects)).  In that situation, we inadvertently mixed two populations, both of which were in Hardy-Weinberg Equilibrium, creating a new synthetic population consisting of individuals from two functionally separate populations. If this synthetic group were to mix, their offspring would contain mixtures of chromosomes from both ancestral populations, as depicted in Figure \@ref(fig:admixture).

```{r admixture, echo=FALSE, fig.cap="Hypothetical pedigree of an individual whose parents come from two separate populations.  Stretches of chromosome are depicted in differing colors to highlight parentage as well as demonstrate admixture in the offspring."}
knitr::include_graphics("media/Admixture.png")
```


By far, the most work on admixture has been done in human populations.  A recent paper by Hellenthall *et al.* (2014) introduced an online [Genetic Atlas of Human Admixture History](http://admixturemap.paintmychromosomes.com/) whereby you can examine how the extent to which populations at particular geographic locations are admixed by various populations and the date (or at least a range of dates with a confidence interval around them) are provided for when admixture had occurred.  You should go play with this Atlas, it is worth the time to see how it works and what kinds of inferences can be gained from it.  In our preferred data set, arapat, we can see that there is the potential for quite a bit of differences in allele frequencies between the large regional 'Species' groups.  

An ubiquitous feature of modern population genetic analyses is the presence of a model-based clustering analysis such as those provided by programs such as STRUCTURE and TESS.  Here, they assume a particular population genetic model and estimate admixture and its inverse—the number of groups present in your data that are not admixed.  If you have a reason to believe that that there are divisions in your data that separate groups of samples into discrete partitions, this is a great way to evaluate underlying statistical support based upon your observed data.  

The general idea here is the current distribution of genotypes ($X$) within individuals and populations is due to historical mixing of populations ($Z$), at specified mixing rates ($Q$), whose allele frequency spectra ($P$) may differ.  

Problematically, we can write this as:

\[
P(X|Z,P,Q) = \prod_{i=1}^N\prod_{j=1}^LP_{z_{ij},\;p_j,\;x_{ij}}
\]

Estimating the particular values $Z$, $P$, and $Q$ (realized for each of the $N$ individuals across $L$ loci), has to be done numerically. There are an infinite number of combinations for these parameters that have an exceedingly low probability of producing a set of genotypes like we see in our data sets.  However, it is possible to have a set of parameters that produced data much like what we see.  If we tentatively accept these parameters as being representative of the admixture history of our data, we can then quantify the degree of admixture in our own individuals to these hypothetical historical populations.  This assignment process can provide some valuable insights to partitioning in the data, presumably due to ongoing or historical processes, particularly if the consequence of these processes result in defined ‘edges' or partitions in your data.  If you data are distributed along a gradient, these approaches are not quite as powerful.

In this example, I'll use the more common software STRUCTURE to demonstrate how this technique is used.  There is an R interface to this software that you can use to run batch jobs of these analyses which I'll show at the end.  There are other programs available but this is the most simple implementation. 

To use STRUCTURE, you need to reformat your data. Population designations cannot be anything other than numeric values, alleles have to be encoded in a particular way, files need to be formatted in a particular way, etc.  Total pain.  Fortunately, we can do all this as:

```{r eval=FALSE}
write_population(arapat, file="arapat_structure.txt", 
                 mode="structure", stratum="Population")
```

which will take your R data.frame with loci and other information in it and save it as a text file in the appropriate directory in the STRUCTURE format.  

You run this program using either the command-line approach or via a GUI that comes with it.  Most people will prefer the GUI.  After starting it, do the following:

Create a folder for the project. For some reason the Java GUI that STRUCTURE uses disallows the creation of new folders on OSX (don't know if it does the same on windows). Put your data file (the `arapat_structure.txt` file) in that folder. 

Create a new project in STRUCTURE using $File \to New\;Project$:

Walk through the setup pages. The salient numerical information you will need is:  
- Number of Individuals: 363 
- Number of loci: 8
- Missing data value: -9

You should also check the following boxes:  
- Individual ID for each individual 
- Putative population origin for each individual

This should have you set up and ready to go. For an initial run through the data, you need to specify some parameters for the simulations. You do this by selecting $Parameter\;Set \to New$. 

For demonstration purposes, I'll run it kind of low since we do not want to wait around all day. The default values on the parameters are a good place to start, though you have to put in the Length of the burn in and the number of reps; 100,000 & 10,000 should get you started so that you see how the program runs. You can name this parameter set whatever you like.

To make an actual run, you need to hit the run button (it has the exclamation mark on it). It will ask you how many groups are you going to run, input 3 and let it go. It will chug along for a while, dumping out some output on the bottom of the interface. When finished it will give you an output of the run showing the parameter set. 

On the left, select the Results folder and then the $K=3$ option to see the specifics of that particular run.  Salient information on the output includes:

- The number of individuals per cluster before analyses (by population assignment),  
- The allele frequency differences per cluster,  
- The probability of the data give K=3, and  
- The allele frequencies of each cluster

Perhaps more interpretive are the plots of individuals as they were assigned to each group.  You can visualize this by selecting the Bar Plot menu at the top of the Simulation Results pane. 

If you select it to Sort by $Q$ it should look something like Figure below.

This figure is based upon 363 columns of admixture information, one column for each individual.  The colors are defined by the number of groups, here $K=3$.  You can see some individual columns (=individuals genotypes) who are entirely one color.  These are individuals whose genotypes suggest they are the most likely from that colored group.  You can also see that there are some individuals who may be admixed between two or even three groups ad indicated by a column with more than one color.

```{r structureByQ, echo=FALSE, fig.cap="Output from STRUCTURE using the *Araptus attenuatus* when setting $K=3$."}
knitr::include_graphics("media/structure_by_q.png")
```

This output is a hypothesis about admixture.  As such, it is only one simulation and as we've done many times thus far, we should probably run several of these to generate a level of confidence for any value we specified as $K$.  Here is where it gets a little dicey.  We specified $K=3$ and as such we found out what the $P(X|Z,P,Q)$ by specifying $K=3$ implicitly.  In the output of our data, we see can find the log likelihood of our data given these parameters.  However, $K$ may be some value other than three.  Running this simulation with specified values only tells us the likelihood of the data for that value it does not determine if the specified $K$ is the correct one.  

Inferences on which $K$ is actually correct can only be made by running the program for several iterations for each value of $K$ (to understand the variability in the simulations for that particular value) and running several different values for $K$ itself so we can compare the probability of the data we observed for different numbers of clusters.

The data below depict some runs of the data for $K$ assuming values ranging from 2 to 6.  These are the raw values for the probability of the data for specific values of $K$.  If you are doing this on your data for real, you should do more than three runs, but this provides a base approach for understanding the output.


```{r message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Probability of the observed data for the numbers of clusters ($K$) ranging from 2 to 6 using STRUCTURE.  A total of 10 replicates were run for each level of $K$."}
library(ggplot2)
K <- c(2,2,2,3,3,3,4,4,4,5,5,5,6,6,6)
LData <- c( -4885.4, -4882.3,-4892.8, 
            -4256.6,-4261.3,-4367.9, 
            -3659.7, -3657.1, -3660.4, 
            -3605.2, -3594.0,-3607.3,
            -3308.7,-3492.7,-3493.7)
df <- data.frame( K=factor(K), Prob=LData )
ggplot(df, aes(x=K,y=Prob)) + geom_boxplot() + xlab("Number of Clusters (K)") + ylab("Ln Probability of the Data")
```

The output shows that as we assume more clusters, the $log(P(Data))$ tends towards an asymptote.  There are a couple things to look at here for this output, and this is where the interpretive power of you the researcher needs to step in. The overall notion among many is that the way in which the probability of the data changes with an increasing number of clusters should be informative as to the ‘correct' number of clusters found. This may or may not be true depending upon your data, your sampling, and the actual history of your organism. However, if it were true then the rationale suggests that when the probability of the data levels off, that may be a good place to look at other sources of inference to see if this may be supported as a plausible number of clusters. In the Figure, this looks like $K=4$ may be a good place to start.

If I go back to STRUCTURE and plot examine the barplot of individuals from the $K=4$ data (setting "Group by PopID"), you see something like Figure \@ref(fig:structureByPop).

```{r structureByPop, echo=FALSE, fig.cap="STRUCTURE output for *Araptus attenuatus* for $K=4$ sorted by population."}
knitr::include_graphics("media/structure_by_pop.png")
```


Here they are sorted into populations and colored by group (green, yellow, blue, red).  From this display, we can make a few inferences: 

There is a pretty good indication that some of the ‘populations' that I sampled appear to be filled with individuals of a single definite type.

If I look at the groupings as it is presented, the 1^^st^^, 2^^nd^^, and 21^^st^^ populations mainly consist of a single type (the green type). If I look at the original data set that we put out, these correspond to populations 101, 102, and 32. These are the three mainland populations that have consistently been found to be different.

The rest are populations in Peninsular Baja.  As we saw in the hierarchical clustering example, these populations may be further subdivided into different groups, Cape vs. the rest and perhaps more nested structure therein.  In actuality, if you pull out the mtDNA types for Peninsula only and run STRUCTURE on it, you will find there is most likely a three separate groupings in the data (this is where the Cluster column in the arapat data set comes from).

Determining the proper $K$ is not a trivial thing.  From the output of estimates of $P(X|Z,P,Q)$ given a particular $K$, we can examine how our data probability changes with different values of $K$.  We can also examine the allocation of individuals into groups and see if it makes sense, biologically.  

One way that is commonly used to get an idea of the magnitude of $K$ is by looking at how $P(X|Z,P,Q)$ changes with increasing values of $K$.  This is often referred to as the $\delta K$ approach.  There is an argument to be made that the best assignment of $K$ would be where there is a large change in $\delta K$ followed by a plateau.  This is interpretive and not a definite heuristic.  

```{r deltaK, echo=FALSE, fig.cap="Estimates of $\\delta K$ for the STRUCTURE output run on *Araptus attenuatus*."}
mean.K <- as.numeric(by( df$Prob, df$K, mean))
delta.K <- mean.K[2:5] - mean.K[1:4]
df <- data.frame( K=3:6, deltaK=delta.K)
ggplot(df,aes(x=K,y=deltaK)) + geom_line() + geom_point() + xlab("Number of Clusters (K)") + ylab("Change in Ln Probability of the Data")
```

What we see is that the largest value of $\delta K$ (Figure \@ref(fig:deltaK)) followed by a steep reduction is when $K=4$.  This may indicate to us that as it stands, the totality of the data contains four groups and we can map them back onto our data set and evaluate the biological support for these groupings.

The arapat data set itself has a column in it for Cluster, which is derived from an approach such as this.  What we found is that the Cape & Mainland populations were definitely separated from the Peninsular ones.  However, when we examined only the peninsular populations, we found roughly three groups contained (nested) within that main group.  One of those groups was so prominent as to be able to be identified in this analysis causing $K=4$ instead of $K=3$ (Mainland, Cape, Peninsula).  However, the distinctness of the Cape and Mainland groups hid the further sub-structuring in the Peninsula populations.  This is an example of how you should perhaps not just grab the first result you get from any analysis, but make sure the totality of your analyses are considered before jumping to biological conclusions.


## Ordination

There are several ways that we can look for natural groupings in our data.  Barriers and sources of vicariance across the landscape create discontinuities in the genetic structure.  As we begin to analyze the spatial structure of the data itself, one of the first things that needs to be done is to allow the data to tell the analyst how it is internally structured.  This is a deviation from common approaches in statistical analyses.  Typically, we start with a model and then see the extent to which the data fit into a model framework, potentially testing many different models.  However, just because you get a significant model does not mean it is the best or even the correct one describing the underlying processes.  In Dyer & Nason (2004), we examined the spatial genetic structure of Lophocereus schottii (Cactaceae) and found that while the Sea of Cortéz has acted to separate mainland populations from those on peninsular Baja California, there is a particular population at the very northern extent of the range that is really a peninsular population even though it is spatially on the mainland.  The differences, in aggregate, between peninsula and mainland populations are so large that even if we allocated an individual stratum into the wrong region, we still were able to derive a significant model.  This is where model-free approaches are most powerful, they let the data tell us how it is distributed as opposed to asking if there is enough structure to fit into some pre-conceived model.

A popular way to examine how individuals and populations are configured is to use an ordination approach or apply a multivariate rotation on the data.  In what follows, I'll describe a principal components analysis (though it is not really an analysis, it is more of a projection).  These approaches are common in multivariate statistics and have very little to do with population genetic processes or assumptions.  Fortunately, there are several approaches amenable to transforming population genetic data into a form that is usable in normal multivariate statistics, the one used most often is discretizing the data.  To demonstrate, consider the case where we have four individuals genotyped for a single locus as depicted below.

```{r ordination1, message=FALSE, warning=FALSE}
library(gstudio)
x <- c( locus(c(1,2)), locus( c(1,1) ), locus(c(1,3)), locus(c(3,3)))
x
```

We can convert these data into a matrix of data, where each row in the matrix represents an individual and each column determines the frequency of alleles in that individuals genotype.  In this case, the resulting matrix should have four rows (one for each individual) and three columns (one for each of the three alleles present in the whole data).  This conversion is done using the function `to_mv()`.


```{r}
X <- to_mv(x)
X
```

When using multiple loci, we can essentially bind onto this matrix additional columns of predictor variables.  This matrix can be used as an input to most multivariate analyses.  Depending upon the analysis being used, you may need to modify it a bit.  Many statistical approaches require that you invert the covariance matrix.  For example, the regression coefficients are found as:

\[
\beta = (X'X)^{-1}X'Y
\]

where the $X$ matrix is the predictor data and the $Y$ is the response variables.  That $(\cdot)^{-1}$ exponent represents inversion of the matrix resulting from the multiplication.  The problem with the above [~] matrix derived from the genotypes is that the rows are all linearly related in that they all sum to unity.

```{r}
rowSums(X)
```

which is a problem for matrix inversion as it throws a zero into the mix and dividing by zero is still a problematic endeavor.  To fix this, we need to drop a column from each locus we use to estimate.  This is an optional flag in the `to_mv()` function.

```{r}
X <- to_mv(x,drop.allele = TRUE)
X
```

Once we have the proper data, we can do a principal component (PC) rotation.  A PC rotation is one that takes the original columns of data and performs a rotation on the values to align onto new ‘synthetic' axes.  Consider the example in the next figure.  Here, some bivariate data is plot in 2-space, though this can be done for much higher dimensions of data as well—in fact it is more beneficial with more columns of data and this can be used as a way of reducing the dimensionality of the data while loosing very little (or no) content (more later on this).  The axes of a PC rotation are taken as linear combinations of the existing axes and define a new coordinate set onto which the points are plot.  All points are rigidly constrained to keep the same relationship and there is no loss of information.  The PC axes are defined by determining the most variable stretch through the data. In the figure on the next page, we see the raw data plot onto the X- and Y-axes.  The axis of highest variance does not align with either of the original ones, and instead can be defined as a combination of both X- and Y- coordinates.  If we take the blue axis as the first PC axis, the coordinate of the points would be taken along that new synthetic axis.  The next PC axis is defined as being perpendicular to the previous one(s) and is identified as covering the largest variance in the data as before.  This process continues until there are no more axes.  In our case, the second axis would be at a right angle from the blue line (above).  You can, at maximum, have as many PC axes as there are columns of data.  However, the later axes may not explain any significant chunk of the underlying data, the process of rotating based upon axes of maximal variation may be able to capture the complete dataset with fewer axes than the total set.  This is where a technique like this may be helpful in reducing the dimensionality of the data.

To perform this on genetic data, we first translate the raw genotypes into multivariate data, dropping an allele from each locus.

```{r}
data(arapat)
x <- to_mv(arapat, drop.allele = TRUE)
```

The full arapat data, with 363 individuals has 50 independent allele columns to it, after dropping one for each locus. Passing this to the princomp() function we get the results.


```{r}
fit.pca <- princomp(x, cor = TRUE)
```


Here are the first 8 (out of 50 potential) axes for the arapat data set.

```{r}
summary(fit.pca)
```

This output has two important components to it.  First, it shows the axes, in decreasing order of importance and how much of the total variation they describe.  The first Comp.1 axis explains 14.2% of the variance, the second explains 9.5%, etc.  Second, it shows the cumulative proportion of the variation explained.  From the 50 axes we started with, we can explain 49% of the variance by using just the first eight PC axes.

Where this becomes meaningful for us is in how we can project our original data onto these new coordinate locations and look at the distribution to see if there are any obvious trends, partitions, gradients, etc.  

```{r warning=FALSE, message=FALSE}
require(ggplot2)
pred <- predict(fit.pca)
df <- data.frame(PC1 = pred[, 1], PC2 = pred[, 2])
df$Species <- arapat$Species
df$Clade <- arapat$Cluster
df$Pop = arapat$Population
ggplot(df) + geom_point(aes(x = PC1, y = PC2, shape = Species, color = Clade), size = 3, alpha = 0.75)
```

We can see from the plot (I've added some designations to the points) that the 363 samples are clustered in an obvious way.  The designation of ‘Species' as depicted by the shape of the points, is defined by the mtDNA clade for each individual, independent of the nuclear marker data we are using here.  Still, it shows a broad separation between the Cape, Mainland, and Peninsula groups.  The colors of the points found within the Peninsula group, come more formal clustering approaches, as defined in the next two sections.
 



## Hierarchical Clustering

In the previous section, we defined a new coordinate space for all individuals in the dataset.  The rotation of the 50 allele encoding columns was able to describe over 95% of the observed variation using only the first 34 PC axes.  In this section, we are going to use the rotated coordinates to evaluate population-level differences using a hierarchical clustering method.  Hierarchical clustering are very helpful in understanding groupings in the data, particularly if there is a ‘nesting' structure.  While there are many ways to do it, they all generally proceed as follows:  
1. Define a numeric metric that measured the distances between all K groups.  
2. Find the two groups that have the smallest distance and coalesce them together into a pair.  
3. Assume that the coalesced pair now constitutes a single entity, estimate the numeric metric among all K-1 groups in the data set.  
4. Go to #2 and repeat until you have coalesced all the groups together.

Here again, it is the data that is telling us how it is structured rather than us imposing a model onto the data to see if it fits.

To do this, the rotated coordinates are used to define the centroid of each population.  Here I use the tapply() function as a short-cut to estimate the mean of each population.  If you are not familiar with this approach, it essentially applies a particular function (in this case taking the mean), to a set of data  in a matrix (the predicted coordinates from the PCA) based upon a set of factor levels (the populations).  This is quite an efficient way to do what would take you a bit of looping to get through.

```{r}
p <- ncol(pred)
pops <- arapat$Population
pop.means <- tapply( pred, list(rep(pops,p),col(pred)),mean)
dim(pop.means)
```

The result is a matrix where each row represents a population and each column represents the mean location of all individuals for each of the 50 PCA axes variables.  These 50-dimensional coordinates can be used to define a pairwise distance metric using the dist() function.  By default, this measures the euclidean distance (e.g., straight-line distance) between each populations 50-dimensional coordinate.

```{r}
pop_pw <- dist(pop.means)
h <- hclust( pop_pw)
plot(h)
```

The plot of this shows the nesting structure of the populations as depicted in the PC-derived data.  

There are some interesting things to notice here.   
- The main separation between mainland populations (32, 101, 102) and peninsular populations (the rest) seems to be the largest difference.  As indicated in the mtDNA (not used for this analysis), there seems to be a clear division between those two groups, consistent with the idea that the Sea of Cortéz has acted as a source of vicariance.
- There seems to be a deep division within the peninsular populations separating out the group of populations including 98, Mat, 157, 73, 75, Aqu, ESan, 156, and 48.  If we look at where these populations are found, we see that they are all located in the southern portion of Baja California—the Cape groupings...
- Then there is the rest of the populations, some of which appear to be partitioned into at least two groups, though there may be more.

This approach is pretty good at pulling apart components within the data set that represent different genetic clusters.

## Interactive Tree Display

Another network approach that could be used is defining bifurcating networks.  Here we will examine the *A. attenuatus* data set using Nei's genetic distance metric.

\[
D_{nei} = -\ln( L )
\]

where

\[
L = \frac{\sum_l\sum_u x_uy_u}{\sqrt{(\sum_l\sum_u x_u^2)(\sum_l\sum_u y_u^2)}}
\]

I make the among stratum distance matrix and then turn it into a `dist` object to do a hierarchal clustering on the populations. I then plot it and make the node colors equal to the grouping of the dendrogram *if* there were 4 groups in the dataset.  This is also zoomable, for large trees, so if you want to figure out the relationships at the leaves, where the internal edges are smaller, zoom in with the mouse.

```{r message=FALSE, error=FALSE, warning=FALSE}
require(networkD3)
d <- genetic_distance(arapat,stratum = "Population", mode="Nei")
d <- as.dist( d )
hc <- hclust(d)
dendroNetwork(hc, height=600, 
              zoom=TRUE, 
              textColour = c("red","green","orange","blue")[ cutree(hc,4)])
```



## Mantel Tests

```{r}
mixed <- c("157","163","164","166","73","75","77","98","Aqu","Const","ESan","Mat")
data <- droplevels( arapat[ !(arapat$Population %in% mixed),] )
D <- genetic_distance(data,mode="Nei")
coords <- strata_coordinates( data )
P <- strata_distance( coords )
D <- D[ lower.tri(D) ]
P <- P[ lower.tri(P) ]
rho <- cor(D,P)
Z <- rep(NA,1000)
Z[1] <- sum(P*D)
for(i in 2:1000) {
  P <- sample( P, size=length(P), replace = FALSE)
  Z[i] <- sum(P*D)
}
df <- data.frame( Z, Value=c("Observed",rep("Permuted",999)))
cdf <- sum( Z<Z[1] )/1000
colors <- c("#5ab4ac","#d8b365")
ggplot( df, aes(x=Z,fill=Value)) + geom_histogram(bins=30) + ylab("Frequency") + theme_bw()  + scale_fill_manual(name = "", values = colors) 
```



```{r}
library(vegan)
clusters <- unique( cbind( data$Cluster, data$Population))
C <- outer(clusters[,1], clusters[,1], function(x,y) return( ifelse( x == y, 1, 2 ) ))
P <- strata_distance( coords )
D <- genetic_distance(data,mode="Nei")
mantel.partial(as.dist(D),as.dist(C),as.dist(P))
mantel.partial(as.dist(D),as.dist(P),as.dist(C))
```



```{r}
library(raster)
library(sp)
pts <- strata_coordinates(data,as.SpatialPoints = TRUE)
alt <- raster("spatial_data/alt.tif")
pts$elev <- extract(alt, pts)
pts <- pts[ !is.na(pts$elev),]
coords <- coords[ !is.na(pts$elev),]
P <- strata_distance(coords)
elev <- outer(pts$elev, pts$elev, function(x,y) return ( (x - y)^2 ))
lat <- outer( coords$Latitutde, coords$Latitude, function(x,y) return ( (x - y)^2 ))

df1 <- data.frame( Latitude= coordinates(pts)[,2], Elevation=pts$elev)
ggplot( df1, aes(Latitude, Elevation)) + geom_point() + theme_bw()
cor(df1$Latitude, df1$Elevation )
```

```{r}
elev <- outer(df1[,2], df1[,2], function(x,y) return ( (x - y)^2 ))
lat <- outer( df1[,1], df1[,1], function(x,y) return ( (x - y)^2 ))
df2 <- data.frame( Elevation = elev[ lower.tri(elev)], Latitude= lat[ lower.tri(lat)] )
ggplot( df2, aes(Latitude, Elevation)) + geom_point() + theme_bw() + xlab("Pairwise Latitude") + ylab("Pairwise Elevation")
cor( df2$Latitude, df2$Elevation )
```


### Autocorrealtion problem with mantel


```{r}
X <- rnorm(1296)
Y <- rnorm(1296)

rX <- raster(matrix(X,nrow=36))
rY <- raster(matrix(Y,nrow=36))

plot(rX,legend=FALSE,axes=FALSE, box=FALSE)
plot(rY,legend=FALSE,axes=FALSE, box=FALSE)
rho.p <- rep(NA,10000)
fit <- cor.test(X,Y)
K <- 50

for( i in 1:10000){
  idx <- sample( 1:2500, size=K, replace=FALSE)
  ct <- cor.test(X[idx], Y[idx])
  rho[i] <- ct$p.value
}
type1 <- length(rho[ rho<0.05])/1000
type1
```



```{r}
xr <- raster(ncols=50, nrows=50, xmn=0)
xr[] <- runif(ncell(xr))
sX <- focal(xr, w=matrix(1/2,nrow=15,ncol=15))

plot(sX, legend=FALSE,axes=FALSE, box=FALSE)

yr <- raster(ncols=50, nrows=50, xmn=0)
yr[] <- runif(ncell(yr))
sY <- focal(yr, w=matrix(1/9,nrow=15,ncol=15))
plot(sY, legend=FALSE,axes=FALSE, box=FALSE)

vX <- values(sX)
vY <- values(sY)
vXp <- vX[ !is.na(vX) & !is.na(vY)]
vYp <- vY[ !is.na(vX) & !is.na(vY)]

rho.sp <- rep(NA,10000)
for( i in 1:10000){
  idx <- sample( 1:2116, size=K, replace=FALSE)
  ct <- cor.test(vXp[idx], vYp[idx])
  rho.sp[i] <- ct$p.value
}
type1.sp <- length(rho.sp[ rho.sp<0.05])/1000
type1.sp
```


```{r}
par( mfrow=c(2,2))
plot(rX,legend=FALSE)
plot(rY,legend=FALSE)
plot(sX, legend=FALSE)
plot(sY, legend=FALSE)
```





```{r}
D <- genetic_distance(data,mode="Nei")
coords <- strata_coordinates( data )
pts <- strata_coordinates(data,as.SpatialPoints = TRUE)
elev <- extract(alt, pts)
coords <- coords[ !is.na(elev),]
elev <- elev[ !is.na(elev)]
P <- strata_distance( coords )
w <- 1/P
diag(w) <- 0
ape::Moran.I(elev,w)

hist(w)
hist(elev)
qplot( elev, geom="histogram", bins=20) + xlab("Elevation") + theme_bw() + ylab("Frequency")
```


### Raster Autocorrelation

```{r}
alt <- raster("./spatial_data/alt.tif")
pts <- rbind( c(-111.5,27.0),
              c(-112.4,26.7),
              c(-111.7,25.7),
              c(-111.1,25.4),
              c(-110.8,26.0),
              c(-111.5,27.0) )
polys <- SpatialPolygons(list(Polygons(list(Polygon(pts)),"Polygon")))
loredo <- trim( mask( alt, polys ) )
plot(loredo, xlab="Longitude",ylab="Latitude")
```


```{r}
Moran(loredo)
```


```{r}
ml <- MoranLocal(loredo)
plot(ml, xlab="Longitude",ylab="Latitude")
```


